---
linkTitle: 12-09-Daily
title: 12-09-Daily AI News Daily
weight: 23
breadcrumbs: false
comments: true
description: 'Kuaishou Keling''s Subject Library: One Image, Many Angles! ‚ú®'
---
```
## AI Insights Daily 2025/12/9
>  `AI Daily`
### **Today's Digest**
### **Today's AI News**
Keling's Subject Library boosts character consistency and slashes video e-commerce costs, while Alibaba's virtual humans now support long-duration live streaming.
Security research is heating up, with focus on online injection prevention, language region localization, hallucinated citations, and long-memory models.
Smart programming and open-source projects are on the rise, leading to job displacement alongside new opportunities. Mastering AI tools is becoming a career prerequisite.
1.  **Kuaishou Keling's Subject Library: One Image, Many Angles! ‚ú®**
    **Kuaishou Keling's Subject Library** feature is a game-changer. Just upload one image to their `O1` model, and boom! You get multi-angle, multi-lighting, and cross-scene variations, with character consistency hitting a whopping **96%**. The system auto-extracts style keywords. The `Pro` version costs **29 RMB/month**, letting production teams batch-generate storyboards and cutting merchant try-on video costs by **1/10**. Multi-person collaboration is coming next quarter. For video teams and e-commerce folks, this is a direct cost-cutter and efficiency booster. Definitely worth keeping an eye on!
2.  **Perplexity BrowseSafe: 91% Prompt Injection Defense, Even CR7 Invested! ‚öΩ**
    **Perplexity BrowseSafe** is here, boasting a **91%** prompt injection attack interception rate thanks to its three-layer defense mechanism‚Äîthat's 6 percentage points higher than `GPT‚Äë5`. They've also open-sourced their benchmarks and models. Cristiano Ronaldo has announced his investment and signed on as a global ambassador, with plans for a fan interaction hub. However, its detection rate for multi-language attacks is currently only **76%**. For those of you constantly online, using large models for research or coding, this is a crucial layer of security, but you'll still gotta be careful in non-English scenarios.
3.  **Stanford's `CS146S`: No Coding Allowed, AI All The Way! üßë‚Äçüíª**
    **Stanford's new `CS146S` course** mandates students develop software using `Cursor` and `Claude`, and they even have to submit chat logs with their assignments. The waitlist? It's already piled up to **[200+ people]**! This 10-week course covers coding agents, terminal automation, and security vulnerability detection. Instructor Eric, who previously worked with Stanford's `NLP` group, will launch a public version for professional developers next year. For students and programmers eager to get hands-on with "AI pair programming," this practical course totally hits the spot and is definitely one to watch.
4.  **ChatGPT Subscription Trick: Hit "[Unsubscribe]" and Get [1 Month Free Use] Plus! üí∞**
    Here's a neat **ChatGPT subscription trick**: if you go to your `Web` account settings and click **Unsubscribe**, the system might pop up an option for **[1 month free use]**. Multiple overseas users have confirmed this works for `Plus` plans, but you gotta do it in a browser. Currently, it's only verified for individual accounts. For students and light users, this is a money-saving hack to extend your `Plus` subscription. But whether this trick will last is anyone's guess; it's a "[grab it while you can, then watch for policy changes]" kind of deal.
5.  **Alibaba's Live Avatar: Real-Time Virtual Human Streaming, Over [3 Hours] Without Crashing! ü§ñ**
    **Alibaba's Live Avatar** is out, supporting speech-driven virtual humans at **[20 frames/second]** and capable of continuous operation for **[over 3 hours]**. The system uses a three-layer anti-drift mechanism to maintain stable character appearance, combining with the `Qwen3` model for bidirectional language and expression interaction. It employs streaming block generation, allowing student models to approach teacher model quality through self-reinforcement training. The paper and code are already public. For teams looking to create virtual human content and long-duration interactive scenarios, this is a ready-to-use tech stack, totally worth diving into and experimenting with.
6.  **MIT Discovers a "Brain Language Chip": [Strawberry-Sized] Yet Decoupled from Thought! üß†**
    **MIT's 'brain language chip' discovery** comes from **[15 years, 1400 fMRI scans]** of research, pinpointing the human brain's language network to a **4.2cm¬≥** ([strawberry-sized]) area in the left inferior frontal gyrus. Analysis of **[212 aphasia patients]** proves that language and thought modules can be completely decoupled. The corresponding probabilistic map has been open-sourced, and `Meta` and `DeepMind` have already cited this map to optimize large model architectures and brain-computer interface layouts. A dual-region stimulation protocol is set to be released next `Q2`. For researchers in cognitive science, large models, and brain-computer interfaces, this is a foundational, hardcore achievement.
7.  **ICLR 2026 Reveals [50 Instances] of "Hallucinated Citations," AI Papers Are Crashing and Burning! ‚ö†Ô∏è**
    **ICLR 2026's 'hallucinated citations'** are making waves. A research team sampled **300 submissions** and found **[50 instances]** of completely untraceable fabricated references, estimating hundreds of "hallucinated citations" among **[20,000 submissions]**. The current debate centers on how to divide **[author responsibility]** and tool accountability. The community suggests using `BibTeX` validation and `RAG` retrieval, but the detection tool `GPTZero` itself has faced questions about false positives. For students and researchers using `AI` to write papers, this is a red-line risk. Always double-check your references yourself; don't just blame the model.
8.  **Google Titans: Paper-Only "[Inference-Time Memory Architecture]" (No Models Released!) üß©**
    **Google's Titans [inference-time memory architecture]** has been unveiled. It uses gradients as a "[surprise signal]" to instantly update memory modules, supporting self-modifying learning in ultra-long contexts, and achieves layered persistent memory via the `HOPE` scheme combined with the `CMS` system. However, once again, they've only released the paper, not the weights, drawing criticism for contrasting sharply with the open strategies of `Meta` and `DeepSeek`. This also sparks safety discussions around data poisoning and alignment issues. For developers aiming to build long-memory agents and knowledge base applications, this is a direction worth tracking, but for now, you'll just have to read the papers and sketch out prototypes.
9.  **VLM Self-Evolution: 11B Model Outperforms 90B and `GPT‚Äë4o` on Reward Benchmarks! üöÄ**
    Inna Wanyin Lin introduced a **VLM self-improvement framework**. By synthesizing multimodal instruction pairs and generating reasoning trajectories, it boosted the `Llama‚Äë3.2‚Äë11B` score on `VL‚ÄëRewardBench` from **0.38 to 0.51**. This significantly improved hallucination and reasoning dimensions, with overall performance surpassing both **`90B` models and `GPT‚Äë4o`**. The iterative process includes quality grading and self-filtering. For developers and researchers working on multimodal models and reinforcement evaluation systems, this "[self-improvement without human labeling]" approach is absolutely worth replicating.
10. **Open-Source Trio: `VibeSDK`, `Open Notebook`, `Claude Demo` ‚Äì Clone 'Em and Get Hands-On! üíª**
    This **open-source trio** is ready for action! Cloudflare's **`VibeSDK`** (‚≠ê3.6k) is an open-source "[ambient coding]" platform built on the Cloudflare tech stack, offering a complete deployment solution perfect for teams to set up custom coding environments. **`Open Notebook`** (‚≠ê13k) is an open-source alternative to `NotebookLM`, supporting multi-language interfaces, a plugin system, and custom note-taking workflows, making it ideal for private deployment by research teams and educational institutions. Anthropic's **`Claude API` Quickstart Projects** (‚≠ê11.4k) provide deployable examples and detailed best practices for chatbots, document processing, and more. For developers, these three repos are top-notch projects you can clone right now to get hands-on.
11. **2030 Job Warning: 800 Million Jobs Replaced, But 130 Million New Opportunities! üíº**
    **The 2030 job market forecast** from McKinsey predicts `AI` could replace up to **[800 million jobs]** by **2030**, while simultaneously creating **[130 million]** new positions. Brookings research indicates that within a decade, the U.S. could see **[1.3 million to 2.4 million]** job displacements, affecting sectors like driving, logistics, accounting, and healthcare. A Berkeley professor warns that all professions, including `CEO`s, will feel the impact, and an `IBM` executive flat-out stated, "Managers who don't use `AI` will be eliminated." For workers and students, "knowing how to use `AI` tools" is already a "[mandatory course]." If you don't learn it now, you'll struggle to catch up later.
```
## AI Insights Daily 2025/12/9

> `AI Daily`

### **Today's Digest**

```
Keling's Subject Library boosts character consistency and slashes video e-commerce costs, while Alibaba's virtual humans now support long-duration live streaming.  
Security research is heating up, with focus on online injection prevention, language region localization, hallucinated citations, and long-memory models.  
Smart programming and open-source projects are on the rise, leading to job displacement alongside new opportunities. Mastering AI tools is becoming a career prerequisite.
```

### **Today's AI News**

1.  **Kuaishou Keling's Subject Library: One Image, Many Angles! ‚ú®**
    **Kuaishou Keling's Subject Library** feature is a game-changer. Just upload one image to their `O1` model, and boom! You get multi-angle, multi-lighting, and cross-scene variations, with character consistency hitting a whopping **96%**. The system auto-extracts style keywords. The `Pro` version costs **29 RMB/month**, letting production teams batch-generate storyboards and cutting merchant try-on video costs by **1/10**. Multi-person collaboration is coming next quarter. For video teams and e-commerce folks, this is a direct cost-cutter and efficiency booster. Definitely worth keeping an eye on!

2.  **Perplexity BrowseSafe: 91% Prompt Injection Defense, Even CR7 Invested! ‚öΩ**
    **Perplexity BrowseSafe** is here, boasting a **91%** prompt injection attack interception rate thanks to its three-layer defense mechanism‚Äîthat's 6 percentage points higher than `GPT‚Äë5`. They've also open-sourced their benchmarks and models. Cristiano Ronaldo has announced his investment and signed on as a global ambassador, with plans for a fan interaction hub. However, its detection rate for multi-language attacks is currently only **76%**. For those of you constantly online, using large models for research or coding, this is a crucial layer of security, but you'll still gotta be careful in non-English scenarios.

3.  **Stanford's `CS146S`: No Coding Allowed, AI All The Way! üßë‚Äçüíª**
    **Stanford's new `CS146S` course** mandates students develop software using `Cursor` and `Claude`, and they even have to submit chat logs with their assignments. The waitlist? It's already piled up to **[200+ people]**! This 10-week course covers coding agents, terminal automation, and security vulnerability detection. Instructor Eric, who previously worked with Stanford's `NLP` group, will launch a public version for professional developers next year. For students and programmers eager to get hands-on with "AI pair programming," this practical course totally hits the spot and is definitely one to watch.

4.  **ChatGPT Subscription Trick: Hit "[Unsubscribe]" and Get [1 Month Free Use] Plus! üí∞**
    Here's a neat **ChatGPT subscription trick**: if you go to your `Web` account settings and click **Unsubscribe**, the system might pop up an option for **[1 month free use]**. Multiple overseas users have confirmed this works for `Plus` plans, but you gotta do it in a browser. Currently, it's only verified for individual accounts. For students and light users, this is a money-saving hack to extend your `Plus` subscription. But whether this trick will last is anyone's guess; it's a "[grab it while you can, then watch for policy changes]" kind of deal.

5.  **Alibaba's Live Avatar: Real-Time Virtual Human Streaming, Over [3 Hours] Without Crashing! ü§ñ**
    **Alibaba's Live Avatar** is out, supporting speech-driven virtual humans at **[20 frames/second]** and capable of continuous operation for **[over 3 hours]**. The system uses a three-layer anti-drift mechanism to maintain stable character appearance, combining with the `Qwen3` model for bidirectional language and expression interaction. It employs streaming block generation, allowing student models to approach teacher model quality through self-reinforcement training. The paper and code are already public. For teams looking to create virtual human content and long-duration interactive scenarios, this is a ready-to-use tech stack, totally worth diving into and experimenting with.

6.  **MIT Discovers a "Brain Language Chip": [Strawberry-Sized] Yet Decoupled from Thought! üß†**
    **MIT's 'brain language chip' discovery** comes from **[15 years, 1400 fMRI scans]** of research, pinpointing the human brain's language network to a **4.2cm¬≥** ([strawberry-sized]) area in the left inferior frontal gyrus. Analysis of **[212 aphasia patients]** proves that language and thought modules can be completely decoupled. The corresponding probabilistic map has been open-sourced, and `Meta` and `DeepMind` have already cited this map to optimize large model architectures and brain-computer interface layouts. A dual-region stimulation protocol is set to be released next `Q2`. For researchers in cognitive science, large models, and brain-computer interfaces, this is a foundational, hardcore achievement.

7.  **ICLR 2026 Reveals [50 Instances] of "Hallucinated Citations," AI Papers Are Crashing and Burning! ‚ö†Ô∏è**
    **ICLR 2026's 'hallucinated citations'** are making waves. A research team sampled **300 submissions** and found **[50 instances]** of completely untraceable fabricated references, estimating hundreds of "hallucinated citations" among **[20,000 submissions]**. The current debate centers on how to divide **[author responsibility]** and tool accountability. The community suggests using `BibTeX` validation and `RAG` retrieval, but the detection tool `GPTZero` itself has faced questions about false positives. For students and researchers using `AI` to write papers, this is a red-line risk. Always double-check your references yourself; don't just blame the model.

8.  **Google Titans: Paper-Only "[Inference-Time Memory Architecture]" (No Models Released!) üß©**
    **Google's Titans [inference-time memory architecture]** has been unveiled. It uses gradients as a "[surprise signal]" to instantly update memory modules, supporting self-modifying learning in ultra-long contexts, and achieves layered persistent memory via the `HOPE` scheme combined with the `CMS` system. However, once again, they've only released the paper, not the weights, drawing criticism for contrasting sharply with the open strategies of `Meta` and `DeepSeek`. This also sparks safety discussions around data poisoning and alignment issues. For developers aiming to build long-memory agents and knowledge base applications, this is a direction worth tracking, but for now, you'll just have to read the papers and sketch out prototypes.

9.  **VLM Self-Evolution: 11B Model Outperforms 90B and `GPT‚Äë4o` on Reward Benchmarks! üöÄ**
    Inna Wanyin Lin introduced a **VLM self-improvement framework**. By synthesizing multimodal instruction pairs and generating reasoning trajectories, it boosted the `Llama‚Äë3.2‚Äë11B` score on `VL‚ÄëRewardBench` from **0.38 to 0.51**. This significantly improved hallucination and reasoning dimensions, with overall performance surpassing both **`90B` models and `GPT‚Äë4o`**. The iterative process includes quality grading and self-filtering. For developers and researchers working on multimodal models and reinforcement evaluation systems, this "[self-improvement without human labeling]" approach is absolutely worth replicating.

10. **Open-Source Trio: `VibeSDK`, `Open Notebook`, `Claude Demo` ‚Äì Clone 'Em and Get Hands-On! üíª**
    This **open-source trio** is ready for action! Cloudflare's **`VibeSDK`** (‚≠ê3.6k) is an open-source "[ambient coding]" platform built on the Cloudflare tech stack, offering a complete deployment solution perfect for teams to set up custom coding environments. **`Open Notebook`** (‚≠ê13k) is an open-source alternative to `NotebookLM`, supporting multi-language interfaces, a plugin system, and custom note-taking workflows, making it ideal for private deployment by research teams and educational institutions. Anthropic's **`Claude API` Quickstart Projects** (‚≠ê11.4k) provide deployable examples and detailed best practices for chatbots, document processing, and more. For developers, these three repos are top-notch projects you can clone right now to get hands-on.

11. **2030 Job Warning: 800 Million Jobs Replaced, But 130 Million New Opportunities! üíº**
    **The 2030 job market forecast** from McKinsey predicts `AI` could replace up to **[800 million jobs]** by **2030**, while simultaneously creating **[130 million]** new positions. Brookings research indicates that within a decade, the U.S. could see **[1.3 million to 2.4 million]** job displacements, affecting sectors like driving, logistics, accounting, and healthcare. A Berkeley professor warns that all professions, including `CEO`s, will feel the impact, and an `IBM` executive flat-out stated, "Managers who don't use `AI` will be eliminated." For workers and students, "knowing how to use `AI` tools" is already a "[mandatory course]." If you don't learn it now, you'll struggle to catch up later.